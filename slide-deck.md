Capstone: A Predictive Text App
========================================================
author: Graham Booth
date:  16/06/2022




Introducing Capstone
========================================================
- this presentation aims to describe the usage/function of our predictive text app  [Capstone](http://sidechained.shinyapps.io/DataScienceCapstone) (click to view)
- the app was created for the final project of the Data Science Specialisation (Coursera/Johns Hopkins University)
- it is based on data provided by Microsoft SwiftKey
- usage is simple/self-explanatory:
  - user types in text box
  - after a space, prediction of the next word happens 
  - predicted words appear on buttons (click to insert)

About the App
========================================================

- created using Shiny (hosted on shinyapps.io)
- can run in any standard brower
- uses a custom predictive text model/algorithm, which is:
  - pre-calculated for speed/efficiency/low memory usage
  - generated by a separate R script ("corpus_generator.r")
  - stored as a compressed .csv file ("corpus.csv.gz")
- the next slides detail how the model was developed...

About the Predictive Text Model (1)
========================================================

- model trained on English text data gathered by SwiftKey
- uses a random sample (5%) of blogs, news & tweets sets
- using the _tidytext_ library, text data is tokenised into 2 and 3 word ngrams (representing the relationship between words):

```r
unnest_tokens(ngram_text, line, token = "ngrams", n = 2) # or n = 3
```

- each ngram is then split into:
  - a _predicted_ element (the last word of the ngram) and
  - a _predictor_ element (the preceding word/2-words)


```r
extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$')
```

About the Predictive Text Model (2)
========================================================

- next we remove rows where the predicted word is
  - profane, not english or a 'stop word' (i.e. "is", "and" etc)


```r
filter(!predicted %in% profanity$terms) %>% 
filter(predicted %in% GradyAugmented) %>% 
filter(!predicted %in% stop_words$word) 
```

- for each predictor element, we:
  - count the number of occurrences of each predicted word
  - keep (up to) the three most frequent
- these are our 3 best guesses at the next word


```r
group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice_head(n = 3) %>% select(-n)
mutate(id = row_number()) %>% pivot_wider(names_from = c(id), values_from = c(predicted), names_prefix = "predicted")
```

About the Predictive Text Model (3)
========================================================

An extract from the text prediction model .csv file looks like this:
<table>
 <thead>
  <tr>
   <th style="text-align:left;"> predictor </th>
   <th style="text-align:left;"> predicted1 </th>
   <th style="text-align:left;"> predicted2 </th>
   <th style="text-align:left;"> predicted3 </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> on </td>
   <td style="text-align:left;"> twitter </td>
   <td style="text-align:left;"> friday </td>
   <td style="text-align:left;"> top </td>
  </tr>
  <tr>
   <td style="text-align:left;"> on the </td>
   <td style="text-align:left;"> floor </td>
   <td style="text-align:left;"> road </td>
   <td style="text-align:left;"> ground </td>
  </tr>
</tbody>
</table>

- so when _on_ is entered we get _twitter_, _friday_ & _top_ as predictions
- however when _on the_ is entered we receive the more context dependent predictions _floor_, _road_ and _ground_
- if no results for _on the_ are found, the algorithm backs off to  _on_
- for app efficiency we limit this to two-word context prediction, but this approach could be extended to longer phrases e.g. _on the first [day] [thursday] [of]_ for even more accurate prediction
