str_remove(ph, str_pad(word("testing this shit"), 1, "right"))
ph <- "testing this shit"
str_remove(ph, str_pad(word(ph), 1, "right"))
str_pad(word(ph), 1, "right")
str_pad(word(ph), 2, "right")
str_pad(word(ph), 1)
paste0(word(ph), " ")
ph <- "testing this shit"
str_remove(ph, paste0(word(ph), " "))
ph <- "this shit"
str_remove(ph, paste0(word(ph), " "))
prediction <- data_to_find_in %>%
filter(n == data %>% pull(n)) %>%
filter(predictor == ngram) %>%
slice(1)
# modify to detect number of words (can be got from n)
library(rlang)
sbo <- function(data_to_find_in, ngram_to_find, n_level) {
prediction <- data_to_find_in %>%
filter(n == data %>% pull(n)) %>%
filter(predictor == ngram) %>%
slice(1)
predicted <- prediction %>% pull(predicted)
if (is_empty(predicted)) {
ngram_to_find <- str_remove(ngram_to_find, paste0(word(ngram_to_find), " "))
sbo(data_to_find_in, ngram_to_find, n_level-1)
}
else {
probability <- prediction %>% pull(freq)
return(probability)
}
}
testgram_predicted_probabilties <- testgram_predicted %>% sample_n(5) %>% mutate(prob = sbo(testgram_predicted, predictor))
set.seed(123)
testgram_predicted_probabilties <- testgram_predicted %>% sample_n(5) %>% mutate(prob = sbo(testgram_predicted, predictor))
testgram_predicted_probabilties <- testgram_predicted %>%
sample_n(5) %>%
mutate(prob = sbo(testgram_predicted, predictor, n))
traingram %>% filter(n==4) %>% sample_n(1)
traingram %>% filter(n==4) %>% sample_n(1)
?sample_n
testgram_predicted %>%
traingram %>% filter(n==4) %>% slice_sample(1)
traingram %>% filter(n==4) %>% slice_sample(1)
traingram %>% slice_sample(1)
traingram %>% slice_sample(20)
traingram %>% slice_sample(n = 5)
traingram %>% slice_sample(n = 5)
traingram %>% slice_sample(n = 5, replace = T)
mtcars %>% slice_sample(n = 5)
traingram %>% slice_sample(n = 5)
traingram
mtcars
traingram
raingram %>% ungroup
traingram %>% ungroup
traingram %>% ungroup %>% slice_sample(n = 5)
View(testgram_predicted_only)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
set.seed(123)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
View(trainsample)
set.seed(1244)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
View(trainsample)
set.seed(1229)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
View(trainsample)
set.seed(2229)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
View(trainsample)
set.seed(2429)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
View(trainsample)
set.seed(2599)
trainsample <- traingram %>% ungroup %>% slice_sample(n = 5)
View(trainsample)
getwd()
shiny::runApp('DataScience/10_DataScienceCapstone/Week7-FinalProjectSubmission/DataScienceCapstone')
corpus <- read.csv("corpus.csv.gz", header = TRUE)
corpus <- read.csv("corpus.csv.gz", header = TRUE)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
corpus <- corpus %>% sample_n(nrows(corpus)/10)
corpus <- corpus %>% sample_n(n()/10)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
corpus2 <- corpus %>% sample_n(n()/10)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
corpus <- corpus %>% sample_n(n()/1000)
rm(test_blogs, test_news, test_tweets)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
corpus <- corpus %>% sample_n(n()/10000)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
View(corpus)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123)
corpus <- corpus %>% sample_n(n()/100000)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
??separate
corpus2 <- corpus %>% separate(ngram_txt, c("1", "2", "3"), remove = TRUE)
corpus2 <- corpus %>% separate(ngram_text, c("1", "2", "3"), remove = TRUE)
View(corpus2)
View(corpus)
View(corpus2)
corpus2 <- corpus %>% separate(ngram_text, c("1", "2", "3"), sep = " ", remove = TRUE)
View(corpus)
View(corpus2)
View(corpus)
View(corpus2)
View(corpus)
corpus$ngram_text[834]
corpus$ngram_text[835]
View(corpus)
View(corpus2)
corpus2 %>% mutate(x1 = ifelse(is.na("3"), "3", "2"))
corpus3 <- corpus2 %>% mutate(x1 = ifelse(is.na("3"), "3", "2"))
View(corpus3)
corpus2 <- corpus %>% separate(ngram_text, c("one", "two", "three"), sep = " ", remove = TRUE)
corpus3 <- corpus2 %>% mutate(x1 = ifelse(is.na(three), three, two))
View(corpus3)
View(corpus3)
View(corpus2)
?group_by
?select
corpus2 <- corpus %>% separate(ngram_text, c("one", "two", "three"), sep = " $", remove = TRUE)
View(corpus2)
corpus2 <- corpus %>% extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$')
View(corpus2)
rm(corpus2, corpus3)
rm(test_blogs, test_news, test_tweets)
#filter(!(grepl("\\d", ngram_text) %>% # remove digits
#filter(!predicted %in% stop_words$word) %>%
corpus <- corpus %>% extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$')
?count
corpus2 <- corpus %>% count(predicted)
View(corpus2)
corpus2 <- corpus %>% count(predictor, predicted)
View(corpus2)
View(corpus2)
corpus2 <- corpus %>% add_count(predictor, predicted)
View(corpus2)
?group_by
corpus2 <- corpus %>% count(predictor)
View(corpus2)
corpus2 <- corpus %>% add_count(predictor)
View(corpus2)
corpus2 <- corpus %>% add_count(predictor, predicted)
View(corpus2)
corpus2 <- corpus %>% count(predictor, predicted)
View(corpus2)
corpus2 <- corpus %>% count(predictor, predicted, sort = TRUE)
View(corpus2)
?count
?add_count
corpus2 <- corpus %>% group_by(predictor) %>% count(predicted)
View(corpus2)
corpus2 <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE)
View(corpus2)
corpus3 <- corpus2 %>% slice(1)
View(corpus3)
View(corpus3)
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice(1) %>% select(-n)
View(corpus)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123)
corpus <- corpus %>% sample_n(n()/100000)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
corpus <- corpus %>% count(predictor, predicted, sort = TRUE)
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice(1) %>% select(-n)
#filter(!(grepl("\\d", ngram_text) %>% # remove digits
#filter(!predicted %in% stop_words$word) %>%
write.csv(corpus, file = gzfile("corpus.csv.gz"), row.names = FALSE)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123)
corpus <- corpus %>% sample_n(n()/10)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
corpus <- corpus %>% count(predictor, predicted, sort = TRUE)
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice(1) %>% select(-n)
#filter(!(grepl("\\d", ngram_text) %>% # remove digits
#filter(!predicted %in% stop_words$word) %>%
write.csv(corpus, file = gzfile("corpus.csv.gz"), row.names = FALSE)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123)
corpus <- corpus %>% sample_n(n()/10)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice(1) %>% select(-n)
#filter(!(grepl("\\d", ngram_text) %>% # remove digits
#filter(!predicted %in% stop_words$word) %>%
write.csv(corpus, file = gzfile("corpus.csv.gz"), row.names = FALSE)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123)
corpus <- corpus %>% sample_n(n()/100000)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
corpus <- corpus %>% extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$')
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice(1) %>% select(-n)
#filter(!(grepl("\\d", ngram_text) %>% # remove digits
#filter(!predicted %in% stop_words$word) %>%
write.csv(corpus, file = gzfile("corpus.csv.gz"), row.names = FALSE)
View(corpus)
library(shiny)
library(tidyverse)
library(tidytext)
library(stringr)
data(stop_words)
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
# 60000
# todo, sample lines at random?
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
#blogs 889288
#news 1010242
#tweets 1398101
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123)
corpus <- corpus %>% sample_n(n()/10)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
corpus <- corpus %>% extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$')
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice(1) %>% select(-n)
#filter(!(grepl("\\d", ngram_text) %>% # remove digits
#filter(!predicted %in% stop_words$word) %>%
#remove elements with .
write.csv(corpus, file = gzfile("corpus.csv.gz"), row.names = FALSE)
shiny::runApp('DataScience/10_DataScienceCapstone/Week7-FinalProjectSubmission/DataScienceCapstone')
library(tidyverse)
library(tidytext)
library(stringr) # str_detect etc
library(lexicon) # profanity
library(qdapDictionaries) # language filtering
data(stop_words)
data(profanity_alvarez)
data(profanity_arr_bad)
data(profanity_banned)
data(profanity_racist)
data(profanity_zac_anger)
profanity_lexicon <- tibble(
terms = c(profanity_alvarez, profanity_arr_bad, profanity_banned, profanity_racist, profanity_zac_anger)
) %>% filter(!str_detect(terms, " "))
basePath <- "~/DataScience/10_DataScienceCapstone/" # set to where your downloaded "final" folder is
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123) # ensure same lines are sampled, for now
corpus <- corpus %>% sample_n(n()/20)
rm(test_blogs, test_news, test_tweets)
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
corpus <- corpus %>% filter(!is.na(ngram_text))
corpus <- corpus %>% filter(!(grepl("\\d", ngram_text))) # remove rows containing digits
corpus <- corpus %>% filter(!(grepl("\\.", ngram_text))) # remove rows containing full stops
corpus <- corpus %>% extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$') # pretty fast
corpus <- corpus %>% filter(!predicted %in% profanity_lexicon$terms) # remove rows where any predicted value would be a profane term (profanity still allowed in predictor)
corpus <- corpus %>% filter(predicted %in% GradyAugmented) # remove rows where predicted value not in dictionary
corpus <- corpus %>% filter(!predicted %in% stop_words$word) # remove rows where predicted word is a stop word - IS THIS GOOD
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice_head(n = 3) %>% select(-n) # slow
corpus <- corpus %>% mutate(id = row_number())  %>% pivot_wider(names_from = c(id), values_from = c(predicted), names_prefix = "predicted")
write.csv(corpus, file = gzfile("corpus.csv.gz"), row.names = FALSE)
#start_time <- Sys.time()
#
#end_time <- Sys.time()
#print(end_time - start_time)
# sample <- bind_rows(
#   test_blogs %>% sample_n(5),
#   test_news %>% sample_n(5),
#   test_tweets %>% sample_n(5)
# )
#
# write.csv(sample, file = "sample.csv", row.names = FALSE)
shiny::runApp()
