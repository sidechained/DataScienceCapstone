Capstone: A Predictive Text App
========================================================
author: Graham Booth
date:  16/06/2022
autosize: true
```{r include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidytext)
library(stringr) # str_detect etc
library(lexicon) # profanity
library(qdapDictionaries) # language filtering
data(stop_words)
data(profanity_alvarez)
data(profanity_arr_bad)
data(profanity_banned)
data(profanity_racist)
data(profanity_zac_anger)
profanity_lexicon <- tibble(
terms = c(profanity_alvarez, profanity_arr_bad, profanity_banned, profanity_racist, profanity_zac_anger)
) %>% filter(!str_detect(terms, " "))
basePath <- "~/DataScience/10_DataScienceCapstone/"
if (!file.exists(paste0(basePath, "final"))) {
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")
}
test_blogs <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.blogs.txt")))
test_news <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.news.txt")))
test_tweets <- tibble(line = read_lines(paste0(basePath, "final/en_US/en_US.twitter.txt")))
corpus <- bind_rows(
test_blogs,
test_news,
test_tweets,
)
set.seed(123) # ensure same lines are sampled, for now
corpus <- corpus %>% sample_n(n()/100000)
rm(test_blogs, test_news, test_tweets)
```
Introducing Capstone
========================================================
- this presentation aims to describe the usage/function of our predictive text app  [Capstone](http://sidechained.shinyapps.io/DataScienceCapstone) (click to view)
- the app was created for the Capstone project of the Data Science Specialisation
- and is based on data provided by Microsoft SwiftKey
- usage is simple/self-explanatory:
- user types in text box
- after a space, prediction of the next word happens
- predicted words appear on buttons (click to insert)
About the App
========================================================
- created using Shiny (hosted on shinyapps.io)
- can run in any standard brower
- the algorithm for the predictive text model is:
- pre-calculated for speed/efficiency/low memory usage
- generated by a separate R script ("corpus_generator.r")
- stored as a compressed .csv file ("corpus.csv.gz")
- the next slides detail how the model was developed...
About the Predictive Text Model (1)
========================================================
- model trained on English text data gathered by SwiftKey
- uses a random sample (5%) of blogs, news & tweets datasets
- text data is tokenised into 2 and 3 word ngrams (representing the relationship between words), as follows:
```{r echo = TRUE, results = 'hide'}
corpus <- bind_rows(
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 3),
corpus %>% unnest_tokens(ngram_text, line, token = "ngrams", n = 2)
)
```
```{r include = FALSE, message = FALSE, warning = FALSE}
corpus <- corpus %>% filter(!is.na(ngram_text))
corpus <- corpus %>% filter(!(grepl("\\d", ngram_text)))
corpus <- corpus %>% filter(!(grepl("\\.", ngram_text)))
```
About the Predictive Text Model (2)
========================================================
- each ngram is then split into:
- a 'predicted' element (the last word of the ngram) and
- a 'predictor' element (the word or two-words preceding it)
```{r echo = TRUE, results = 'hide'}
corpus <- corpus %>% extract(ngram_text, into = c('predictor', 'predicted'), '(.*)\\s+([^ ]+)$')
```
- next we remove rows where the predicted word is: profane, not english or a 'stop word' (i.e. "is", "and" etc)
```{r echo = TRUE, results = 'hide'}
corpus <- corpus %>%
filter(!predicted %in% profanity_lexicon$terms) %>%
filter(predicted %in% GradyAugmented) %>%
filter(!predicted %in% stop_words$word)
```
- for each predictor element, we:
- count the number of occurrences of each predicted word
- keep (up to) the three most frequent
- these represent our three best guesses at the next word, to be displayed for the user to select from
```{r echo = TRUE, results = 'hide'}
corpus <- corpus %>% group_by(predictor) %>% count(predicted, sort = TRUE) %>% slice_head(n = 3) %>% select(-n)
corpus <- corpus %>% mutate(id = row_number())  %>% pivot_wider(names_from = c(id), values_from = c(predicted), names_prefix = "predicted")
```
About the Predictive Text Model (3)
========================================================
```{r echo = FALSE}
extract <- corpus %>% slice(20:25)
knitr::kable(extract, "html")
```
- when the user types "energy" we receive the predictions "efficiency" and "efficient"
- however when the user types "save energy" we receive the more context dependent predictions "by", "using" or "everyday"
- for now we limit this two-word context prediction, as anything more produces a far larger database, which currently exceeds Shiny App memory limitations
- using other platforms, this approach could be extended to longer phrases e.g. "how to save energy" for even more accurate prediction
extrat
extract
corpus %>% slice(20:25)
corpus %>% slice(4)
corpus
corpus %>% slice(5)
corpus
corpus %>% slice(5L)
head(corpus, 5)
slice(corpus, -(1:4))
slice(corpus, 1:3)
corpus %>% slice(1:3)
corpus[1:3,]
View(corpus)
View(corpus)
View(corpus)
View(corpus)
shiny::runApp('DataScienceCapstone')
?textAreaInput
runApp('DataScienceCapstone')
View(corpus)
View(corpus)
View(corpus)
extract <- corpus[318,321,]
extract <- corpus[318,321]
extract <- corpus[318:321,]
extract <- corpus[318,]
extract <- bind_rows(corpus[318,], corpus[321,])
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 397763, n_max = 1)
?read.csv
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 397763, nrows = 1)
View(line1)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 397762, nrows = 1)
View(line1)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 397761, nrows = 1)
View(line1)
line2 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 400099, nrows = 1)
View(line2)
View(line1)
View(line1)
View(extract)
View(extract)
View(extract)
View(line2)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 397761, nrows = 1)
line2 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 400099, nrows = 1)
extract <- bind_rows(line1, line2)
View(extract)
View(line1)
View(line1)
View(line2)
View(line1)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 397761, nrows = 1)
View(line1)
line0 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = TRUE, skip = 0, nrows = 0)
View(line0)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 397761, nrows = 1)
line2 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 400099, nrows = 1)
extract <- bind_rows(line1, line2) %>% `colnames<-`(c("1", "2", "3", "4"))
View(extract)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 397762, nrows = 1)
line2 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 400100, nrows = 1)
extract <- bind_rows(line1, line2) %>% `colnames<-`(c("1", "2", "3", "4"))
View(extract)
View(corpus)
line1 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 397762, nrows = 1)
line2 <- read.csv("DataScienceCapstone/corpus.csv.gz", header = FALSE, skip = 400100, nrows = 1)
extract <- bind_rows(line1, line2) %>% `colnames<-`(c("predictor", "predicted1", "predicted2", "predicted3"))
View(extract)
View(extract)
View(extract)
